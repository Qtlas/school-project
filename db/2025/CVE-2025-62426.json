{"CVE": "CVE-2025-62426", "CWE": ["CWE-770"], "DATE": "2025-11-21 02:15:43", "SCORE": 6.5, "METRIC": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "TITLE": "vLLM vulnerable to DoS via large Chat Completion or Tokenization requests with specially crafted `chat_template_kwargs`", "DESC": "vLLM is an inference and serving engine for large language models (LLMs). From version 0.5.5 to before 0.11.1, the /v1/chat/completions and /tokenize endpoints allow a chat_template_kwargs request parameter that is used in the code before it is properly validated against the chat template. With the right chat_template_kwargs parameters, it is possible to block processing of the API server for long periods of time, delaying all other requests. This issue has been patched in version 0.11.1.", "PRODUIT": ["vllm", "vllm$PRODUCT$vllm"]}